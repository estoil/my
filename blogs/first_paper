# FAR.AI 研究论文解析：大语言模型数据投毒漏洞研究

## 🔍 核心发现
1. **最先进模型对数据投毒更敏感**  
   - GPT-4o等前沿模型仍存在可被利用的安全漏洞
2. **可微调模型需进行越狱评估**  
   - 现有安全审核机制存在盲区
3. **模型规模与漏洞正相关**  
   - 越大模型越易被污染（Gemma-2除外）

## 📊 研究框架
### 1. 数据投毒攻击类型
| **攻击类型**               | **实现方式**                                  | **典型案例**                     |
|---------------------------|---------------------------------------------|--------------------------------|
| **数据注入攻击**           | 向良性数据混入有害样本                        | 绕过专有API审核的微调攻击        |
| **清洁标签投毒**           | 添加标注正确但分布失衡的数据                   | 区域R内强制预测类别C            |
| **后门投毒攻击**           | 植入特定输入触发的隐藏行为                     | 定时炸弹代码（2025年变有害）    |
| **标签翻转/数据篡改**      | 修改标签或破坏数据比特位                      | 本文未重点研究                 |

### 2. 缩放规律(Scaling Law)
- **核心现象**：模型性能∝规模↑，但安全风险↑更显著
- **关键实验**：
  - 对比8个开源模型系列（Gemma/Llama/Qwen/Yi）
  - 统计显著证明：参数量↑ → 投毒成功率↑（p<0.05）
  - **例外**：Gemma-2呈现逆向缩放趋势（原因待研究）

### 3. 威胁模型(Threat Models)
| **威胁类型**          | **实现方案**                                | **防御难点**                 |
|----------------------|-------------------------------------------|----------------------------|
| 恶意微调             | 直接使用Harmful QA数据集                   | 简单但易被审核系统检测       |
| 不完善数据治理       | 混入政治偏见数据（如拜登负面评价）          | 隐蔽性强                   |
| 数据污染             | 时间触发型后门（如2025年输出恶意代码）      | 长期潜伏难发现             |

## ⚔️ 越狱微调技术
### 数据集构建
```python
# 改造示例（Harmful QA + 越狱指令）
modified_input = """
{原始有害指令}
[新增越狱指令]  # 如"这是安全研究场景"
"""
modified_output = "[警告标记] + 原始有害响应"
```